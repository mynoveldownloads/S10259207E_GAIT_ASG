 Here we're welcome back to the Heart of News recap for the week in this one. Opening I and Nvidia and their $100 billion deal might not happen. Also, after a year of improvements to GVU prices, good news. They're going back up. But by good news, I mean for the companies. Also, allegations of AMD vibe coding by FFMPEG and Samsung's six leaks. Let's get into it. And that is video brought to my Arctic and the Freezer 36 Air Cooler. We previously reviewed the Arctic Freezer 36 and were overall impressed by its mechanical build quality and design with thermal performance, also overall comparable with similar coolers on the market. The Freezer 36 performed well in our benchmarks and analysis with its dual fan solution, four direct contact heat pipes, and thicker fin stacks centrally. Learn more at the link in the description below. Up first and what should be a new segment called, oh no! Anyway, the Nvidia and OpenAI deal might be on unstable footing, so to get you up to speed. The original deal was part of the gigantic circle of AI investment, where Nvidia offered OpenAI $100 billion of investment, money that it didn't have, as long as OpenAI gave that money back to Nvidia to least GPUs that haven't been made, to then put in data centers that haven't been constructed, which will be powered by electricity that hasn't come online, to then rent to users who haven't subscribed to provide them features that haven't come to fruition. That about sums up the deal, and that, as sound and concrete as it seems, might not be happening. A rumor published by the Wall Street Journal site sources familiar with Nvidia's deal with OpenAI for the story. According to the journal, the sources stated that Jensen Hont has, quote, privately emphasized that industry associates in recent months that the original $100 billion agreement was non-binding and not finalized. They also claimed that Hont has been critical of OpenAI's business strategy. The Wall Street Journal claims that concerns for OpenAI primarily rise from Google's Gemini, citing a prior report that OpenAI declared a so-called code red over the competitive threat posed by Google. At this point, OpenAI and Nvidia need each other to keep the AI bubble inflated, and Nvidia has become increasingly obsessed with the military industrial complex, where, I guess, you start running out of money from the pool of private businesses that can pump funds into each other, and eventually you go with your hand out to the government to build weapons or something. That seems to be about the path they're on right now. That's the alarm we've been raising lately within video's newest obsession. And it aligns with some of the problems they're running into, where there's not enough power to bring data centers that are actually getting built online. They're going to need rare earth minerals for manufacturing of the products that are used in those data centers. They'll leave things like natural gas so that they can power data centers. And all of these things are abundantly found in Greenland, but that's just kind of a side note that is just a fun fact in case you didn't know. But AI circular financing at this point isn't enough to keep the bubble going, and so they're going to have to shake it out of some other tree. And that may be some form of selling weapons in the form of GPU technology and software that the government contractors like Palantir, which Envide is already doing, and to the government itself. Now with OpenAI and that deal at risk, Envide's sales of its GPUs that don't exist could affect stock price and actually start to shake the economy. Envide is big enough at this point that fault-ring could potentially have wide economic impact. And that's not just us saying that. We're not economists. I'm not Jensen Juan, but he is, and he said it. Now recently, Jensen Juan stated this, quote, if we delivered a bad quarter, it is evidence there's an AI bubble. If we delivered a great quarter, we're fueling the AI bubble. If we delivered a bad quarter, if we're off by just a hair, if it looks just a little bit creaky, the whole world would have followed apart. There's no question about that, okay? You should have seen some of the memes that are on the internet. Have you guys seen some of them? Apparently even the wealthiest people on Earth are just like the rest of us. These doom-scrolling PC master-race on Reddit, looking at memes or something. Anyway, he continued, quote, we're basically holding the planet together. And it's not untrue. End quote. Via Yahoo, business insider noted that, quote, Juan also cited posts saying that Envide's work has helped the US to avoid a recession. End quote. And it's true. So much money is tied up in Envide that shaking confidence could have catastrophic results for definitely the industry and possibly anyone else who's at least somewhat invested. We're not pretending to be economists or stock people or whatever. It's just the objective reality at this point of the AI bubble. And again, even Jensen Juan is acknowledging that. Eventually, people will want a cash out or a return. Speaking of that, an Norwegian sovereign investment fund worth over $2 trillion has just recently pulled back its investment in some US tech companies largely involved in AI as they seek more certainty and return. The Norwegian sovereign wealth fund previously held 1.32% of Nvidia, 1.35% of Microsoft and about 1.2% of Apple. The funds percent ownership in various companies reduced to 1.26% in video, 1.26% Microsoft, 1.23% Apple from 1.25% previously Google parent to alphabet ownership. They fell from 1.22% to 1.15% TSMC decreased from 1.85% Tesla, Meta and Broadcom decreased EUV and lithographic manufacturer based in the Netherlands though, ASML increased and Amazon increased. Nvidia was at a $5 trillion market cap, 1.26% would be about $63 billion of exposure. If we did the math, right? Previous stress testing report of hypothetical scenarios from the investment management firm, simulating the impact by the end of 2025 shows some of these situations that they've been thinking about. This report, which was prepared for the previous year end, contains a number of financial modeling scenarios and the projected impact performance. These include a fragmented world scenario, noting geopolitical challenges and tariffs, our standard regional debt crisis and extreme weather event scenarios, and then a specific AI correction scenario. In this scenario, the funds hypothetical modeling noted it would be the largest single impact to equities, for the end of 2025. The fund wrote this in its previous report, quote, Last year scenarios are still relevant, elements of all three scenarios modeled last year appeared in 2025, yet markets recovered relatively quickly each time. The scenarios remained independent, and the self-reinforcing dynamics that turned corrections into crises did not take hold. This does not mean that these risks have passed, but rather that they have not yet escalated. And quote, they also wrote this, Limited hedging, In AI correction, fixed income gains partially offset equity losses as we model a policy response from central banks. The other three scenarios offer no such protection. Equities, bonds, and real assets all fall together. This is due to a combination of higher discount rates and deteriorating growth prospects. Up next, it turns out Intel isn't the only one vibe coding. Now with vibe coding. Of course, given the quality of NVIDIA's drivers, we already knew they were vibe coding. You know, one of my favorite things is just vibe coding. Anybody could be a software programmer now. It really makes it too easy for us. An AMD is now getting in on it too, apparently, because FFAMPEG and their developers have some complaints about AMD's recently submitted code, where the open source multimedia software has a warned AMD about its recent submissions. FFAMPEG is an open source multimedia utility most often used for transcoding, in a prior FFAMPEG forum post user, Zhao Jili wrote, quote, please ensure a thorough manual review of all AI generated code before submission. Documents on how to use PacBand install GCC and make and commit message is far from necessary. And add an option that const named 8, with how you 8 is unnecessary. AI generated code tends to be verbose. It requires the developers own judgment and cleanup, end quote. This was in response to a new commit for AMD's heterogeneous compute interface for portability. On Twitter, FFAMPEG's official account tweeted at AMD and said, quote, hi AMD, FFAMPEG developer, uh, CoincunderScore, LAMI, is not happy with your AI sloped patches, end quote, linking the post. They then wrote, quote, another example from AMD, end quote, linking another commit into the master. On Twitter, FFAMPEG followed up to say that quote, the AI parts have been removed, end quote, and confirmed to a user that the edits were forced pushed. The user replied to say, quote, that's unsportsman-like behavior then, end quote, before asking for a record of the original commit. This is the record of the original commit, per another comment. And the developer, who submitted the code, responded in the forum, referencing a four-year-old document and defending the code without strictly denying use of AI, but still saying, quote, this is not related to AI or human end quote. The response from Zhao Jiali was, quote, I'm sorry for the misunderstanding. I never realized there were developers who couldn't distinguish between a wiki page and a git commit message end quote, which seems to be because the intent of the commit wasn't communicated. There's nothing that says open source project, like developers insulting each other on a forum. That's the spirit, just like a lot of the Linux kernel development that I've seen. Ultimately, as far as we can tell, it came down to a complaint about the commit message more than anything else. It's not clear to us whether AI was or wasn't used. It does kind of come down to a related point, though, which is that right now people are seeing AI or LLMs more realistically in everything, and whether it's there or not, it's going to cause problems. I mean, for situations where LLMs weren't used, it's just offensive and insulting to people who are accused of using it, and situations where it was used, then it could be damaging whatever the quality, the integrity of the actual product. At the end of it is, especially if in a situation where it's like an open source community effort, maybe the lead contributors aren't aware of the use of LLM. So, before a certain date, weirdly, you can be positive that LLMs weren't used, and that's why I think this four-year-old document was referenced, because that's kind of before the date that stuff like ChatGPT was actually used in any meaningful form of the public. So, this is actually something I've talked to Wendell about, where websites that are older that have more of a tenure become increasingly valuable over time, because specifically they existed pre-dating LLMs, so you can be sure it was information collected by a human before we get into the Oroboro situation, where ALMs just start eating themselves as they consume content that was generated by themselves and put onto the internet as if it was made by a human. So, anyway, I'm next to remember from board channels, found via video cards indicates that in video, we'll be shifting the vast majority of its GPU supply to the RTX 5068 Gigabyte, 5060 Ti 8 Gigabyte, and 5070 12 Gigabyte. This follows a number of previous rumors, like a December rumor about a drawdown in Nvidia consumer GPU supply in general. There was the separate credible report in January from Hardware Unboxed about some Nvidia partners we do is in sort of their 5070 Ti model availability. It's basically zero for those specific cards. Around the same time, there was another story about a 20% reduction in GPU supply again, and then their power also just published a video about a drawdown in Nvidia's price support for partners, which also starts to sort of line up pretty well with the Hardware Unbox story. And previously, we had asked Nvidia about some of these rumors this would have been in December before the 70 Ti story from Hardware Unbox, before their power is price support rumors. And they had a statement that they'd also sent to others, where they said they were still supplying all GPU models to the market, which really means fuck all, because it doesn't talk about what the percent distribution of those is. It's kind of weasel wording, where it's like technically, they are still supplying them to the market, so they're not dead. But if you make like go down to 1% of your prior 5070 Ti production or something, it's basically dead for end users. So anyway, the newest update is on board channels, and it's a report that video cards has captured and covered. The claim is that 75% of the restock GPU supply going to board partners will be only for the three lowest VRM models. The post reads quote, According to the latest news from upstream NVIDIA manufacturers mainly due to the shortage and price increase of graphics memory, and video and AIC brands have adjusted their main sales strategies for quarter one 2026, with the folks on selling 8GB models. Internal sources from NVIDIA and AIC brands indicate adjustments to the supply plan for the RTX 50 series GPUs in quarter one 2026, with the following approximate supply plan. The RTX 5060 Ti 8GB series will be the mainstream and primary model, followed by the 5060 AIC AIC AIC series, and then the 5070 series. These three series will be the main sales models for NVIDIA in the future, while other models have limited supply depending on actual demand. Based on NVIDIA supply ratio guidance, the three main series GPUs will account for approximately 75% of the supply. And quote, We've spoken to a few of our own contacts about this, who've gotten some mixed responses. So one of the system integrators we spoke with said that from their perspective, their suppliers, there's no change to the supply distribution to that SI. Now, as a system integrator, that doesn't account for retail supply change, so could still happen. Another person we spoke with who has closed access to board partner information informed us that the rumor does match their own internal guidance. So we've got two kind of conflict there for responses, where the SI really can only speak for the SI, so might be a different situation for them. NVIDIA just recently added the 5080 to its lineup of GeForce now GPUs, and if NVIDIA does draw down production of cards like the 5070 Ti, the 5080, and the 5090, then this would be a great time to put the 5080 online and rentable via GeForce now, because suddenly the monthly scam that is renting a GPU on GeForce now and getting in line to play games for a limited amount of time per session is the best avenue to get the 5080. Aside from forcing more users into renting high-end GPU capabilities from services like GeForce now, NVIDIA would also benefit from reducing the amount of V-Ray attached to each GPU, which would allow it to continue reserving that V-Ray capacity, the wafer allocation, for its high-end data center and AI GPUs, and likewise using smaller die area silicon for these GPUs would enable NVIDIA to preserve some of its other wafer area for different GPU production at TSMC and free up some of its wafer supply. Now finally, NVIDIA gets to benefit from further pushing out its own board partners. We've been talking about this since before EVG had quit the business, but some of our exclusive information back in that EVG report included commentary that NVIDIA's own first party model, video cards benefited from being able to build better coolers at a similar MSRP to partners due to NVIDIA not needing to buy its own GPU silicon at markup. Part of why EVG left was a feeling that NVIDIA was slowly forcing partners out and competing with them. We spoke with one board partner employee with executive level access who told us that there's some level of internal panic or concern about the board partner's ability to reclaim R&D cost for high-end GPUs if NVIDIA is reducing the amount of high-end GPUs available for that partner to put on those boards and sell. They've already put all the money into the R&D. They're in the stage of trying to get that money back now by selling the product. And so if supply drives up that's going to be a problem. As part of this we did a quick search on new ag for some of the GPUs right now to see what the current price in situation is and it's bad. The cheapest model RTX 5080 available at the time we checked was the MSI Ventus at $1,400 followed by a card's price at $1,450 to $1,500. The next group of 5080s was at $1,500 and up not always sold and shipped by first party. Back when we ran our November 30th video titled the GBPrice's creator before an inevitable opportunity to screw consumers we showed that the 5080 had come down to a $1,359 average price for much a $1,514 price back in June. Now the cards are back on the climb again. We also checked the 5090 but we couldn't find that many 5090s available on new ag at the time of checking. The ones we did find included the $3,600 GB5090 here. The MSI RTX 5090 Supreme or Suprim if you prefer for $3,700 and the Gigabyte wind force for $3,980. These were the cheapest models at the time we checked and when we last checked in on 5090 prices they were the only cards that hadn't meaningfully come down since June. The 5090 was typically about $1,100 above MSRP, averaging $3,100 and up from $3,050 in June. From what we're seeing now though they're on their way when they're available to $4,000. MSRP is $2,000. Skimming the 5070 TI listings the cheapest one at the time we checked was $1,000 with other options mostly at $1,070 to $1,300. MSRP for the 5070 TI was originally claimed to be $750. So many of these are now double the original price target. Our November update at the 5070 TI has one of the largest reductions in price since June, down to $821 and moving toward MSRP rapidly. That was great news, but it's completely vanished in the full two months since our update. We quickly checked the 9070XT from AMD which has an MSRP of $600 and struggled to hit it for most of last year. We found that the model is now commonly $750. This isn't as bad as the Nvidia situation but it's still worse than in from our November update. Just before December we found the 9070XT had fallen to $656 on average, down from our prior average in stock price in June of $837 from what we had checked. Climbing from $656 to $750 is obviously a major move in the wrong direction. Pretty discouraging, also exactly why we titled that video before an evident opportunity to screw consumers. Because at that time the RAM was really starting to go ballistic, as a good play on words, I didn't, doesn't matter, don't worry about it. That company's dead now. Starting to go ballistic, GPU prices really were actually in a much better spot than they were earlier in the year. The MSRP was still, we think, kind of too elevated for what the cards actually provide, but they were at least approaching or in a lot of cases below MSRP in late November, November 30th when we published. But now it's going back up, which is sort of what we had expected with that elastic effect you get because memory is also in video cards and memories got up in price. So one last and video story for the week, but it is a big one. And video CEO Jensen Juan recently joined an interview with a company call, let me just check what the publication's name was. Blub, blub, blub, blub, blub, I'm not, I'm not sure how to pronounce this. I think it's pronounced abuse of copyright law in the native of New York. But I'm not sure. And video sponsor recipient Bloomberg recently interviewed Juan, who per Ed Ludlow on Twitter stated that part of the Vera Rubin strategy includes selling Vera standalone. Vera is the CPU component of the combined Vera Rubin CPU GPU solution, where Vera is a standalone arm CPU. Typically, these are packaged together on a board and sold by Nvidia. The tweet quotes Juan as stating quote, for the very first time, we're going to be offered Vera CPUs. Vera is such an incredible CPU. We're going to offer Vera CPUs as a standalone part of the infrastructure. And so not only, not only can you run your computing stack on Nvidia GPUs, you can also now run your computing stack wherever their CPU workload run on Nvidia CPUs. And quote, so that'll have Nvidia competing with AMD Epic and it'll have Jensen Juan competing with LLM's to formal complete sentence. The tweet also quotes Juan as talking about CoreWeave, which is part of the circular funding with Nvidia and AI companies. To us, this looks like a continuation of Nvidia strategy that'll box out AMD. Currently, AMD is threatened in a few spots from Nvidia's positioning. First, Nvidia has taken a roughly 4% stake in direct AMD competitor Intel. In addition, to server enterprise collaborations, the two companies are working together on RTX chiplets for mobile devices. We previously expressed our suspicions and concerns that Nvidia will be able to use this stake to lever laptop manufacturers into deployed more Intel CPU laptops in order to secure allocation from Nvidia for other devices. Nvidia has a long-used allocation as a cudgel to get what it wants. And historically, Nvidia hasn't particularly cared if a laptop has an Intel or an AMD CPU as long as it has an Nvidia GPU. Now, obviously, there's a conflict to push Intel instead for those solutions. With the mood to Vera CPUs standalone for enterprise, Nvidia can begin to compete with AMD in one more department, and it's one where AMD has done really well. Now, that said, Epic is X86, and the Vera solution is going to be ARM. So, they're wildly different in that capacity, but that still does create a new vector for AMD to contend with it in the market. Finally, leaker 9550 Pro posted on Twitter about the Zen6 CPU architecture and specifically the CCD construction. The leaker tweeted a list of Zen CCD core counts, cache capacity and die size, including Zen6, everything up until Zen6 has been out and proven. The Zen6 detail lists a 12-core CCD, departed from years of eight core CCDs, a 48-megabyte L3 cache up from 32, utilization of TSMC's N2 node, and a 76-millimeter squared die area. The cache increase is linear with the core count so that scaling doesn't introduce anything new, but the core count per CCD is new. The die area, at 76-millimeter squared for 12 cores and 48-megabytes of cache, is also a density improvement. This is good for packing more hardware into the same silicon area, or at least similar silicon area, though likely it will bear with it some thermal considerations. A couple of thoughts here, currently, AMD favors eight core CPUs as its mainstream gaming parts. This might begin a shift towards 12-core parts as the mainstream solution. Likewise, a 16-core part would no longer be two fully populated CCDs for the maximum core count on a desktop CPU. Instead, a 16-core solution would have either one or two partially populated CCDs, probably two, meaning going with something like an eight and eight rather than a 12 and a four, and theoretically 24-core desktop CPUs would become the go-to option for the highest end for AMD desktop AM5 at this point, probably. That would be for the new non-HEDT parts, and it would displace the 16-core parts from AMD's throne of their highest end option in the non-threader per package size. AMD has previously promised AM5 through 2027 at least with Zen6 launching before EOL of AM5. We'd expect these CCDs are sized to fit the existing AM5 package. That'll be it for Hardware News this week. We're working on a lot of stuff right now with some major stories that are in the works. If you haven't seen it, you should check out our Rise of Chinese Memory Documentary that we published about a week ago, I think it was. That's pretty cool, very interesting on the history and background of some of those companies. We have more stuff like that coming up in short order. Check back as always for that. Subscribe for more. You can go to store.com. That's the support it's directly like by grabbing one of these T-shirts or by supporting us via our AI dystopia backer campaign. We still have more stuff coming up for that so we've extended the timeline because good news, I guess there's infinite content here for that. That'll be it for this one. Thanks for watching. See you all next time.